{
  "topic_id": "Functions",
  "topic_name": "Functions",
  "questions": [
    {
      "id": "differencelosscost",
      "type": "mc_single",
      "question": "Whats the difference between \"Loss\" and \"Cost\" function?",
      "explanation": ".",
      "options": [
        "Loss measures error for one sample",
        "Cost measures error for one sample",
        "Loss measures error for all samples",
        "Loss and cost are almost always identical"
      ],
      "correct": 0
    },
    {
      "id": "howabsoluteerrotreatslargeerrors",
      "type": "mc_single",
      "question": "How does absolute error treat large errors?",
      "explanation": ".",
      "options": [
        "It treats all errors linearly",
        "It amplifies large errors strongly",
        "It ignores large errors entirely",
        "It converts errors into probabilities"
      ],
      "correct": 0
    },
    {
      "id": "squareerrormesurment",
      "type": "mc_single",
      "question": "What does the squared error measure?",
      "explanation": ".",
      "options": [
        "It amplifyies difference between values",
        "The absolute difference between values",
        "The ratio between two values",
        "The product of two values"
      ],
      "correct": 0
    },
    {
      "id": "disadvantageofsquareerror",
      "type": "mc_multi",
      "question": "What is disadvantage of using Square error function?",
      "explanation": "Squared error strongly amplifies large errors and creates a curved loss surface that may contain local minima, which can trap optimization before reaching the global minimum.",
      "explanation_image": "Functions/Screenshot 2026-01-11 173842.png",
      "options": [
        "It is very sensitive to outliers",
        "It is sensitive to local extremes",
        "It ignores large prediction errors",
        "It cannot be used for regression",
        "It produces non-continuous gradients"
      ],
      "correct": [
        0,
        1
      ]
    },
    {
      "id": "Disadvantageofabsoluteerror",
      "type": "mc_single",
      "question": "What is a disadvantage of using the absolute error function?",
      "explanation": "The absolute error is not differentiable at zero, which makes gradient-based optimization more difficult compared to squared error.",
      "explanation_image": "Functions/Screenshot 2026-01-11 174249.png",
      "options": [
        "It has non-smooth gradients at zero",
        "It strongly amplifies large errors",
        "It ignores differences between values",
        "It produces many local minima"
      ],
      "correct": 0
    },
    {
      "id": "hubertsloss",
      "type": "mc_single",
      "question": "Which function combines pros of square and absolute error, while getting rid if disadvantages?",
      "explanation": "",
      "explanation_image": "Functions/Screenshot 2026-01-11 174324.png",
      "options": [
        "Hubert Loss",
        "Andrews Loss",
        "Hugo's Loss",
        "Euler Loss",
        "Smooth Gradient Function"
      ],
      "correct": 0
    },
    {
      "id": "relucomparedtotanhandsigmoid",
      "type": "mc_single",
      "question": "Why is ReLU often preferred over Sigmoid or Tanh in deep networks?",
      "explanation": "ReLU does not saturate for positive values, which helps gradients propagate better during backpropagation compared to Sigmoid or Tanh.",
      "options": [
        "It reduces vanishing gradient problems",
        "It limits outputs to a fixed range",
        "It converts values into probabilities",
        "It removes all negative activations"
      ],
      "correct": 0
    },
    {
      "id": "leakyrelu",
      "type": "mc_single",
      "question": "What problem does Leaky ReLU solve compared to standard ReLU?",
      "explanation": ".",
      "options": [
        "It avoids neurons becoming inactive",
        "It forces outputs into a small range",
        "It increases the model input size",
        "It removes all negative outputs"
      ],
      "correct": 0
    },
    {
      "id": "sigmoidtanh",
      "type": "mc_single",
      "question": "What is a key difference between the Sigmoid and Tanh activation functions?",
      "explanation": "",
      "explanation_image": "Functions/Screenshot 2026-01-11 175140.png",
      "options": [
        "Tanh outputs values between −1 and 1",
        "Sigmoid outputs values between −1 and 1",
        "Tanh outputs only positive values",
        "Sigmoid outputs unbounded values"
      ],
      "correct": 0
    },
    {
      "id": "smoothmonotonicfunctions",
      "type": "mc_multi",
      "question": "Which functions are smooth and non-monotonic?",
      "explanation": "",
      "explanation_image": "Functions/2026-01-14-120914_hyprshot.png",
      "options": [
        "Swish",
        "GELU",
        "Mish",
        "ELU",
        "RELU",
        "Leaky RELU",
        "SELU"
      ],
      "correct": [
        0,
        1,
        2
      ]
    },
    {
      "id": "selurecommended architecture",
      "type": "fill_text",
      "question": "Which activation function is recommended for \"self-normalizing architectures\"",
      "explanation": "",
      "explanation_image": "Functions/2026-01-14-121329_hyprshot.png",
      "answers": [
        "selu",
        "SELU",
        "Selu"
      ]
    },
    {
      "id": "whatisembedding",
      "type": "mc_single",
      "question": "What is an embedding in machine learning?",
      "explanation": ".",
      "options": [
        "A learned dense vector representation of discrete objects that captures semantic or structural relationships",
        "A fixed binary vector representation of discrete objects that contains exactly one active position",
        "A numerical vector representation of input samples that directly stores their raw feature values",
        "A compressed vector representation of data created without using any training process",
        "A numerical vector representation used only to calculate loss during neural network training"
      ],
      "correct": 0
    },
    {
      "id": "whatisattention",
      "type": "mc_single",
      "question": "What is attention in neural networks?",
      "explanation": "Attention computes similarity scores between a query and a set of keys and uses them to form a weighted sum of values. This lets the model focus on the most relevant tokens or features instead of treating all inputs equally, which is crucial for sequence modeling and transformers.",
      "options": [
        "A mechanism that allows a model to dynamically weight and select relevant parts of the input when computing an output",
        "A technique that processes all input elements with equal importance during every network layer",
        "A fixed weighting scheme that assigns constant importance values to all input features",
        "A dimensionality reduction method that compresses sequences into a single vector",
        "A regularization technique that prevents neural networks from overfitting"
      ],
      "correct": 0
    },
    {
      "id": "autoregressivemodelsusage",
      "type": "mc_multi",
      "question": "Which of the following are typical use cases of autoregressive models?",
      "explanation": "",
      "explanation_image": "Functions/2026-01-14-124912_hyprshot.png",
      "options": [
        "Natural language",
        "Translation",
        "Time Series",
        "Forecasting",
        "Image  & Video"
      ],
      "correct": [
        0,
        1,
        2,
        3,
        4
      ]
    },
    {
      "id": "kingmanwomanqueen",
      "type": "fill_text",
      "question": "In a word embedding space, ‘king’ − ‘man’ + ‘woman’ is closest to which word?",
      "explanation": ".",
      "answers": [
        "queen",
        "quen",
        "Queen",
        "Quen"
      ]
    }
  ]
}